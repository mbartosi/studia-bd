{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import pyspark\n",
    "import time\n",
    "import datetime\n",
    "import html\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Funkcja pomocnicza: wyszukiwanie podanego atrybutu w linii pliku XML,\n",
    "# zwraca wartość znalezionego atrybutu lub None jeśli atrybut nie istnieje\n",
    "\n",
    "def attribute_search(attribute, string):\n",
    "    result = re.search(attribute + '=\\\"(.*?)\\\"', string)\n",
    "    if result:\n",
    "        return result.group(1).replace('\"', '')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Funkcje pomocnicze: interpretacja plików XML (różne schematy danych)\n",
    "# ze Stackoverflow\n",
    "\n",
    "def tags_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['TagName'] = attribute_search('TagName', c);\n",
    "    count = attribute_search('Count', c);    \n",
    "    row['Count'] = int(count) if count else None;\n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def badges_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['UserId'] = int(attribute_search('UserId', c));\n",
    "    row['Name'] = attribute_search('Name', c);\n",
    "    row['Date'] = datetime.datetime.strptime(attribute_search('Date', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    row['Class'] = int(attribute_search('Class', c));\n",
    "    row['TagBased'] = ast.literal_eval(attribute_search('TagBased', c));\n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def users_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['Reputation'] = int(attribute_search('Reputation', c));\n",
    "    row['CreationDate'] = datetime.datetime.strptime(attribute_search('CreationDate', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    row['DisplayName'] = attribute_search('DisplayName', c);\n",
    "    row['LastAccessDate'] = datetime.datetime.strptime(attribute_search('LastAccessDate', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    row['WebsiteUrl'] = attribute_search('WebsiteUrl', c);\n",
    "    row['Location'] = attribute_search('Location', c);\n",
    "    age = attribute_search('Age', c);\n",
    "    row['Age'] = int(age) if age else None;\n",
    "    row['Views'] = int(attribute_search('Views', c));\n",
    "    row['UpVotes'] = int(attribute_search('UpVotes', c));\n",
    "    row['DownVotes'] = int(attribute_search('DownVotes', c));    \n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def posts_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['PostTypeId'] = int(attribute_search('PostTypeId', c));\n",
    "    found_id = attribute_search('ParentId', c);\n",
    "    row['ParentId'] = int(found_id) if found_id else None;\n",
    "    found_id = attribute_search('AcceptedAnswerId', c);\n",
    "    row['AcceptedAnswerId'] = int(found_id) if found_id else None;    \n",
    "    row['CreationDate'] = datetime.datetime.strptime(attribute_search('CreationDate', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    row['Score'] = int(attribute_search('Score', c));\n",
    "    vc = attribute_search('ViewCount', c);\n",
    "    row['ViewCount'] = int(vc) if vc else None;\n",
    "    owner = attribute_search('OwnerUserId', c);\n",
    "    row['OwnerUserId'] = int(owner) if owner else None;\n",
    "\n",
    "    row['Body'] = re.sub('(<!--.*?-->|<[^>]*>)', '', html.unescape(attribute_search('Body', c)));\n",
    "    title = attribute_search('Title', c);\n",
    "    row['Title'] = title if title else None;\n",
    "    tags = attribute_search('Tags', c);\n",
    "    row['Tags'] = html.unescape(tags).replace('<', '').replace('>', ' ') if tags else None;\n",
    "    date = attribute_search('ClosedDate', c);\n",
    "    row['ClosedDate'] = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S.%f\") if date else None;\n",
    "\n",
    "    count = attribute_search('AnswerCount', c);\n",
    "    row['AnswerCount'] = int(count) if count else None;\n",
    "    count = attribute_search('CommentCount', c);    \n",
    "    row['CommentCount'] = int(count) if count else None;\n",
    "    count = attribute_search('FavoriteCount', c);\n",
    "    row['FavoriteCount'] = int(count) if count else None;\n",
    "        \n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def comments_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()    \n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['PostId'] = int(attribute_search('PostId', c));\n",
    "    row['Score'] = int(attribute_search('Score', c));\n",
    "    row['Text'] = re.sub('(<!--.*?-->|<[^>]*>)', '', html.unescape(attribute_search('Text', c)));\n",
    "    row['CreationDate'] = datetime.datetime.strptime(attribute_search('CreationDate', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    user = attribute_search('UserId', c);\n",
    "    row['UserId'] = int(user) if user else None;\n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def post_history_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['PostHistoryTypeId'] = int(attribute_search('PostHistoryTypeId', c));\n",
    "    row['PostId'] = int(attribute_search('PostId', c));\n",
    "    comm = attribute_search('Comment', c);\n",
    "    row['Comment'] = comm if comm else None;\n",
    "    text = attribute_search('Text', c);\n",
    "    row['Text'] = re.sub('(<!--.*?-->|<[^>]*>)', '', html.unescape(text)) if text else None;    \n",
    "    return pyspark.Row(**row)\n",
    "\n",
    "def post_links_from_xml(line):\n",
    "    c = line.replace('<row', '').replace('/>', '')\n",
    "    row = dict()\n",
    "    row['Id'] = int(attribute_search('Id', c));\n",
    "    row['CreationDate'] = datetime.datetime.strptime(attribute_search('CreationDate', c), \"%Y-%m-%dT%H:%M:%S.%f\");\n",
    "    row['PostId'] = int(attribute_search('PostId', c));\n",
    "    row['RelatedPostId'] = int(attribute_search('RelatedPostId', c));\n",
    "    row['LinkTypeId'] = int(attribute_search('LinkTypeId', c));    \n",
    "    return pyspark.Row(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych do RDD z plików XML, a następnie konwersja RDD do DF: \n",
    "\n",
    "xml_load_path = 'file:///home/marek/Dokumenty/Notebooks/gis_stack_spark/data/'\n",
    "\n",
    "# Słownik: nazwa pliku i odpowiadająca mu funkcja pomocnicza interpretująca\n",
    "# schemat danych w pliku XML\n",
    "xml_load_list = {'Tags.xml': tags_from_xml, 'Badges.xml': badges_from_xml, \\\n",
    "                 'Users.xml': users_from_xml,'Posts.xml': posts_from_xml, \\\n",
    "                 'Comments.xml': comments_from_xml,'PostHistory.xml': post_history_from_xml, \\\n",
    "                 'PostLinks.xml': post_links_from_xml}\n",
    "\n",
    "tags_rdd = sc.textFile(xml_load_path + 'Tags.xml').filter(lambda line: \"row\" in line) \\\n",
    "             .map(lambda l: xml_load_list['Tags.xml'](l))\n",
    "\n",
    "badges_rdd = sc.textFile(xml_load_path + 'Badges.xml').filter(lambda line: \"row\" in line) \\\n",
    "               .map(lambda l: xml_load_list['Badges.xml'](l))\n",
    "\n",
    "users_rdd = sc.textFile(xml_load_path + 'Users.xml').filter(lambda line: \"row\" in line) \\\n",
    "              .map(lambda l: xml_load_list['Users.xml'](l))\n",
    "\n",
    "posts_rdd = sc.textFile(xml_load_path + 'Posts.xml').filter(lambda line: \"row\" in line) \\\n",
    "              .map(lambda l: xml_load_list['Posts.xml'](l))\n",
    "\n",
    "comments_rdd = sc.textFile(xml_load_path + 'Comments.xml').filter(lambda line: \"row\" in line) \\\n",
    "                 .map(lambda l: xml_load_list['Comments.xml'](l))\n",
    "\n",
    "post_history_rdd = sc.textFile(xml_load_path + 'PostHistory.xml').filter(lambda line: \"row\" in line) \\\n",
    "                     .map(lambda l: xml_load_list['PostHistory.xml'](l))\n",
    "\n",
    "post_links_rdd = sc.textFile(xml_load_path + 'PostLinks.xml').filter(lambda line: \"row\" in line) \\\n",
    "                   .map(lambda l: xml_load_list['PostLinks.xml'](l))\n",
    "\n",
    "# Konwersja na DataFrame:\n",
    "users = sqlContext.createDataFrame(users_rdd)\n",
    "badges = sqlContext.createDataFrame(badges_rdd)\n",
    "posts = sqlContext.createDataFrame(posts_rdd)\n",
    "tags = sqlContext.createDataFrame(tags_rdd)\n",
    "comments = sqlContext.createDataFrame(comments_rdd)\n",
    "post_history = sqlContext.createDataFrame(post_history_rdd)\n",
    "post_links = sqlContext.createDataFrame(post_links_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "post_links.printSchema()\n",
    "post_links.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "users.printSchema()\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
